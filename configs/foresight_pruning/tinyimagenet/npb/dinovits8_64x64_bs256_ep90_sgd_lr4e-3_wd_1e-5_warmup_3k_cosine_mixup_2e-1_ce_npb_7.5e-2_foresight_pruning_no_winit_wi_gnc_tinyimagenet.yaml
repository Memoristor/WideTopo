globals:  # global settings
  output_root: ./output  # The path for saving checkpoints, wandb, logs, results
  log_level: info  # The log level for logger, options are `debug`, `info`, `warning`, `error`, and `critical`
model:  # model settings
  cls: DinoViTS8  # The class name of the model, which needs to be included in the `models` package
  kwargs:  # The parameters of the model class
    image_size: 64
    num_classes: 200
  input_keys:  # The keys of the tensor inputs
    - image
  output_keys:  # The keys of the tensor outputs
    - logit
optimizer:  # optimizer settings
  cls: SGD  # The class name of the optimizer, which needs to be included in the `torch.optim` package
  kwargs:  # The parameters of the optimizer class
    lr: 0.004
    momentum: 0.9
    weight_decay: 0.00001
  gradient_norm_clipping: 1.0  # Norm clipping the gradients
learning_rate:  # learninig rate scheduler settings
  step_mode: iteration  # The step mode for learning rate scheduler, options are `epoch`, `iteration`
  warmup_steps: 3000  # Number of steps of warmup strategy, Note that when the number of steps is greater than 0, the total number of steps of the post scheduler should be subtracted
  cls: CosineAnnealingLR  # The class name of the learning rate scheduler, which needs to be included in the `scheduler` package
  kwargs:  # The parameters of the learning rate scheduler class
    T_max: 32190  # 391 x 90 - 3000
    eta_min: 0.0000004
dataset:  # dataset settings
  cls: TinyImageNetDataset  # The class name of the dataset, which needs to be included in the `datasets` package
  root_path: ~/datasets/Tiny-Imagenet  # The root path of the dataset
  ignore_class:  # The ignored class, class name (str) or class index (int) are supported
  num_workers: 8  # How many subprocesses to use for data loading
train:  # training settings
  reload_pth: last_epoch.pth  # The pth file name that required to be loaded for training resuming
  enable_amp: False  # Enable automatic mixed precision
  num_epoch: 90  # The number of training epoch
  batch_size: 256  # The total number of batch sizes for all GPUs
  summary_freq: 400  # The frequency for flushing weight&bias summaries
  dataset:  # The settings to determine how to load training dataset
    kwargs:  # The parameters of the training dataset class
      test_mode: False
      image_dir: train
      image_suffix: .JPEG
    transforms:  # Data transforms included in the `datasets.transforms` package
      func: pad_random_crop_hflip
      kwargs:  # The parameters of the transform function
        image_size: 
          - 64
          - 64
        padding_size: 
          - 4
          - 4
        horizontal_flip_prob: 0.5
  mixup:  # Mixup/Cutmix that applies different params to each element or whole batch
    cls: Mixup  # The class name of the mixup/cutmix, which needs to be included in the `datasets.mixup` package
    kwargs:
      mixup_alpha: 0.8
      cutmix_alpha: 0.0
      prob: 1.0
      mode: batch
      num_classes: 200
  metrics:  # Metric settings for training and validation
    acc:  # Metric key
      cls: MulticlassAccuracy  # # The class name of the metric, which needs to be included in the `torchmetrics` package
      kwargs:  # The parameters of the metric class
        num_classes: 200
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
    iou: 
      cls: MulticlassJaccardIndex
      kwargs: 
        num_classes: 200
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
    f1: 
      cls: MulticlassF1Score
      kwargs: 
        num_classes: 200
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
  losses:  # Loss settings for training and validation
    ce:  # Loss key
      cls: CrossEntropySoftTarget  # The class name of the loss, which needs to be included in the `losses` package 
      kwargs:  # The parameters of the loss class
          size_average: True
      forward:  # The tensors keys for loss forward, e.g. - input/output: key
        - outputs: logit
        - inputs: target
  engine:  # The class name of the running engine, which needs to be included in the `engines` package 
    cls: MaskedClassificationTrain
valid:  # validation settings
  reload_pth: last_epoch.pth  # The pth file name that required to be loaded for valiation
  batch_size: 256  # The total number of batch sizes for all GPUs
  summary_freq: 100  # The frequency for flushing weight&bias summaries
  dataset:  # The settings to determine how to load training dataset
    kwargs:  # The parameters of the validation dataset class
      test_mode: False
      image_dir: val
      image_suffix: .JPEG
    transforms:  # Data transforms included in the `datasets.transforms` package
      func: image_resize_only
      kwargs:  # The parameters of the transform function
        image_size: 
          - 64
          - 64
  metrics:  # Metric settings for training and validation
    acc:  # Metric key
      cls: MulticlassAccuracy  # # The class name of the metric, which needs to be included in the `torchmetrics` package
      kwargs:  # The parameters of the metric class
        num_classes: 200
        average: none
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
    iou: 
      cls: MulticlassJaccardIndex
      kwargs: 
        num_classes: 200
        average: none
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
    f1: 
      cls: MulticlassF1Score
      kwargs: 
        num_classes: 200
        average: none
      tensor_keys:  # The tensors keys for metric update, e.g. - input/output: key
        - outputs: logit
        - inputs: label
  engine:  # The class name of the running engine, which needs to be included in the `engines` package 
    cls: BasicValid
test:  # testing settings
  reload_pth: last_epoch.pth  # The pth file name that required to be loaded for valiation
  batch_size: 256  # The total number of batch sizes for all GPUs
  summary_freq: 100  # The frequency for flushing weight&bias summaries
  dataset:  # The settings to determine how to load training dataset
    kwargs:  # The parameters of the validation dataset class
      test_mode: True
      image_dir: test
      image_suffix: .JPEG
    transforms:  # Data transforms included in the `datasets.transforms` package
      func: image_resize_only
      kwargs:  # The parameters of the transform function
        image_size: 
          - 64
          - 64
  engine:  # The class name of the running engine, which needs to be included in the `engines` package 
    cls: BasicTest
foresight_pruning:  # Foresight pruning settings
  prunable_params: configs/prunable_params/dinovits8_prune_prune_ln2d_linear.yaml  # A yaml file that lists the prunable parameters names 
  batch_size: 256  # The total number of batch sizes for all GPUs
  sample_size: 2000  # Load multiple batches so that each class roughly has ten samples
  model_mode: eval  # SNIP, Iterative SNIP, and NPB prune networks under train mode, while Synflow uses evaluation mode.
  prune_iters: 1  # Number of iterations for scoring
  target_density: 0.075  # Density level of model at the final prune epoch
  density_scheduler: poly  # Density scheduler, `linear`, `cosine`, and `poly` are supported
  mask_type: custom  # Updates masks of model with scores by density level parameter-wise or global-wise, options are `global`, and `local`
  restore_params: True  # After parameter pruning, the remained parameters will be restored to their initialized values
  pruner:  # To determine which pruner shall be used
    keyword: NPB  # The keyword of prunner, which can be shown in wandb as group key
    cls: NPB  # The class name of the model pruner, which needs to be included in the `pruners` package
  engine:  # The class name of the running engine, which needs to be included in the `engines` package 
    cls: ModelPruning